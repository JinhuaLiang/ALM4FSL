# EXPERIMENT SETTING INTERFACE
database: esc50
experiment: ml_fewshot
model_weights_path: /data/EECS-MachineListeningLab/jinhua/ALM4FSL/ckpts/CLAP_weights_2022.pth

# [Optaional] DETAILED EXPERIMENT SETTINGS
fewshot:
  # ALGORITHM
  adapter: match

  # FEWSHOT SETTING
  n_class: 15
  n_supports: 1
  n_queries: 30

  # TRAINING SETTING
  fine_tune: false
  train_epochs: 15
  learning_rate: 0.0001

  a: 1.0  # over_all = a * fewshot_logits + zeroshot_logits
  b: 5.5  # fewshot_logits = torch.exp(- b + b * attention) @ support_onehots

  xattention:
    disturb: true

# DATABASE SETTINGS
esc50:
  audio_dir: /data/EECS-MachineListeningLab/datasets/ESC-50/audio
  csv_path: /data/EECS-MachineListeningLab/datasets/ESC-50/meta/esc50.csv

fsdkaggle18k:
  audio_dir: [
    '/data/EECS-MachineListeningLab/datasets/FSDKaggle2018/FSDKaggle2018.audio_train',
    '/data/EECS-MachineListeningLab/datasets/FSDKaggle2018/FSDKaggle2018.audio_test'
  ]
  csv_path: [
    '/data/EECS-MachineListeningLab/datasets/FSDKaggle2018/FSDKaggle2018.meta/train_post_competition.csv', 
    '/data/EECS-MachineListeningLab/datasets/FSDKaggle2018/FSDKaggle2018.meta/test_post_competition_scoring_clips.csv'
  ]

fsd_fs:
  clip_dir: /data/EECS-MachineListeningLab/datasets/FSD_FS/clips/eval
  audio_dir: /data/EECS-MachineListeningLab/datasets/FSD_FS
  csv_path: /data/EECS-MachineListeningLab/datasets/FSD_FS/meta
  mode: eval # ['dev_base', 'dev_val', 'eval']


# hydra:
#   run:
#     dir: ${OUTPUTS.DIR}